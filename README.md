# üîí Proxmox Secure Test Lab ‚Äî Prompt de construction (v3)

> **Objectif** : Transformer deux vieux laptops en environnement de test s√©curis√© avec Proxmox VE + NAS de backup, document√© et versionn√© sur GitHub pour un portfolio DevOps ‚Äî stack modernis√©e 2025/2026.

---

## üìã Contexte du projet

Je veux cr√©er un **homelab s√©curis√© sur Proxmox VE** install√© sur un vieux laptop (n≈ìud principal), accompagn√© d'un **second laptop servant de NAS/backup**, destin√© √† servir d'environnement de test pour pratiquer l'administration syst√®me, le r√©seau et le DevOps. Le projet doit √™tre **enti√®rement document√©**, **reproductible** via des scripts d'automatisation, et **publi√© sur GitHub** comme projet portfolio.

---

## üíª Hardware disponible

### PC 1 ‚Äî N≈ìud Proxmox principal

| Composant | Spec |
|-----------|------|
| **CPU** | Intel Core i5-8265U (4 cores / 8 threads, 8√®me gen ‚Äî Whiskey Lake) |
| **RAM** | 8 Go DDR4 |
| **Stockage** | 250 Go SSD |
| **VT-x/VT-d** | ‚úÖ Support√© |
| **R√¥le** | Hyperviseur Proxmox VE ‚Äî h√©berge toutes les VMs et CTs |

### PC 2 ‚Äî Serveur NAS / Backup

| Composant | Spec |
|-----------|------|
| **CPU** | Intel Core i5-5200U (2 cores / 4 threads, 5√®me gen ‚Äî Broadwell) |
| **RAM** | 8 Go DDR3L |
| **Stockage** | 1 To HDD |
| **R√¥le** | Debian minimal ‚Äî NFS/Samba + r√©ception des backups vzdump + backup BDD |

---

## ‚ö†Ô∏è Contraintes 8 Go RAM ‚Äî Strat√©gie d'optimisation

Avec seulement **8 Go sur le n≈ìud Proxmox**, chaque Mo compte. Voici les r√®gles :

### Principes

1. **Containers LXC en priorit√©** ‚Äî Un CT consomme 10-50√ó moins qu'une VM (pas de kernel d√©di√©)
2. **Seulement 2 VMs** ‚Äî pfSense (obligatoire, pas de support LXC) + Kali (besoin d'un kernel complet)
3. **Swap configur√©** ‚Äî 4 Go de swap sur le SSD comme filet de s√©curit√©
4. **Pas de surallocation agressive** ‚Äî Garder ~512 Mo de marge pour Proxmox lui-m√™me
5. **Services optionnels √©teints** ‚Äî Kali (VLAN 40) d√©marr√© uniquement quand n√©cessaire

### Budget RAM d√©taill√©

| Composant | Type | RAM allou√©e | Notes |
|-----------|------|-------------|-------|
| **Proxmox VE** | Host | ~1 Go | OS seul, ext4 (pas de ZFS ARC) |
| **pfSense** | VM | 512 Mo | Firewall + DHCP + DNS ‚Äî pr√©voir 768 Mo si IDS/IPS activ√© |
| **Nginx Reverse Proxy** | CT | 256 Mo | L√©ger en tant que proxy |
| **PostgreSQL** | CT | 512 Mo | Raisonnable pour du test |
| **VictoriaMetrics + Grafana** | CT | 512 Mo | VM remplace Prometheus ‚Äî 30-50% moins gourmand en RAM |
| **Forgejo** | CT | 384 Mo | Git self-hosted communautaire (fork actif de Gitea) |
| **Woodpecker CI** | CT | 384 Mo | Fork actif de Drone CI, pipeline YAML compatible |
| **Docker Registry** | CT | 256 Mo | Registry v2 minimal |
| **Kali Linux** | VM | 2 Go | ‚ö° **√âteint par d√©faut** ‚Äî d√©marr√© √† la demande |
| | | **~3,8 Go** | *(sans Kali)* |
| | | **~5,8 Go** | *(avec Kali ‚Äî capacit√© max)* |
| **Marge libre** | ‚Äî | ~2,2 Go | Buffers, cache, pics d'utilisation |

> üí° **Gain vs v2** : ~600 Mo r√©cup√©r√©s gr√¢ce √† VictoriaMetrics (vs Prometheus) et Woodpecker (vs Drone). La marge passe de 1,6 Go √† 2,2 Go.
>
> üí° **Astuce** : Fusionner Forgejo + Woodpecker dans un seul CT est possible pour gagner ~256 Mo si besoin.

---

## üéØ Cahier des charges fonctionnel

### Architecture cible

```
                        [INTERNET]
                            |
                    [PC 1 ‚Äî Proxmox VE]
                            |
                [pfSense VM ‚Äî Firewall/Router]
                    |               |
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    |               |               |               |
 VLAN 10         VLAN 20         VLAN 30         VLAN 40
 Management      Services        CI/CD           DMZ
 10.10.10.0/24   10.10.20.0/24   10.10.30.0/24   10.10.40.0/24
    |               |               |               |
 Proxmox UI      CT Nginx        CT Forgejo      VM Kali
 SSH             CT PostgreSQL   CT Woodpecker    (isol√©)
                 CT Monitoring   CT Registry
                        |
                   [R√©seau local]
                        |
                [PC 2 ‚Äî Debian NAS]
                   NFS / Samba
                Backup vzdump
                Backup pg_dump
                1 To stockage
```

### Exigences techniques

- **Hyperviseur** : Proxmox VE 8.x (derni√®re version stable)
- **Filesystem** : ext4 (un seul SSD de 250 Go ‚Äî ZFS inutile sans mirror)
- **Firewall** : pfSense CE en VM (512 Mo RAM)
- **Segmentation** : 4 VLANs avec r√®gles firewall inter-VLAN
- **Acc√®s distant** : Tailscale sur le n≈ìud Proxmox (l√©ger, pas de config NAT)
- **Automatisation** : Scripts Bash + Ansible pour le provisioning
- **Monitoring** : VictoriaMetrics + Grafana + Node Exporter (dans un seul CT)
- **CI/CD** : Forgejo + Woodpecker CI + Docker Registry priv√©
- **Backup** : vzdump local ‚Üí rsync chiffr√© vers PC 2 (NAS)
- **S√©curit√©** : Fail2ban, SSH hardening, certificats auto-sign√©s, UFW double couche
- **NAS** : Debian 12 minimal sur PC 2 avec NFS + script de r√©ception backup

### Pourquoi cette stack plut√¥t qu'une autre ?

| Choix | Alternative √©cart√©e | Raison |
|-------|---------------------|--------|
| **Forgejo** | Gitea | Fork communautaire, gouvernance ind√©pendante, d√©veloppement plus actif depuis 2023. Gitea est pass√© sous contr√¥le d'une entit√© commerciale. |
| **Woodpecker CI** | Drone CI | Fork open source actif de Drone. Drone est quasi-abandonn√© depuis le rachat par Harness. Syntaxe YAML compatible. |
| **VictoriaMetrics** | Prometheus | Drop-in replacement, 30-50% moins gourmand en RAM/CPU, meilleure compression. PromQL compatible. |
| **ext4** | ZFS | Un seul disque SSD ‚Äî ZFS n'apporte rien sans mirror et consomme de la RAM (ARC). |
| **pfSense VM** | OPNsense / VyOS | pfSense reste la r√©f√©rence la plus document√©e pour un homelab. OPNsense serait aussi valable. |
| **LXC** | Docker everywhere | LXC est natif Proxmox, plus l√©ger, meilleur contr√¥le r√©seau avec les VLANs. Docker tourne dans les CTs quand n√©cessaire (Woodpecker, Registry). |

---

## üèóÔ∏è Plan de r√©alisation ‚Äî Phase par phase

### Phase 0 : Pr√©paration hardware & installation

#### PC 1 ‚Äî Proxmox VE

```
T√¢ches :
1. V√©rifier le BIOS du laptop i5-8265U :
   - Activer VT-x (Intel Virtualization Technology)
   - Activer VT-d (Intel VT for Directed I/O) si disponible
   - D√©sactiver Secure Boot
   - Configurer le boot USB en premier
2. Cr√©er une cl√© USB bootable avec l'ISO Proxmox VE 8.x (Ventoy ou dd)
3. Installer Proxmox VE :
   - Filesystem : ext4 (un seul disque de 250 Go, ZFS inutile sans mirror)
   - Partitionnement : ~220 Go pour les VMs/CTs, ~20 Go pour l'OS, 4 Go swap
   - Hostname : pve-lab.local
   - IP statique : 10.10.10.1/24 (ou DHCP temporairement)
4. Configuration post-install :
   - D√©sactiver le repo entreprise :
     sed -i 's/^deb/#deb/' /etc/apt/sources.list.d/pve-enterprise.list
   - Activer le repo no-subscription :
     echo 'deb http://download.proxmox.com/debian/pve bookworm pve-no-subscription' \
       > /etc/apt/sources.list.d/pve-no-subscription.list
   - Supprimer le popup de souscription (optionnel, QoL) :
     sed -Ei.bak "s/NotFound/Active/g" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js
   - Mettre √† jour : apt update && apt full-upgrade -y
   - Configurer le r√©seau bridge (vmbr0) pour les VLANs
   - Activer le VLAN-aware sur vmbr0
   - Ajouter 4 Go de swap :
     fallocate -l 4G /swapfile && chmod 600 /swapfile
     mkswap /swapfile && swapon /swapfile
     echo '/swapfile none swap sw 0 0' >> /etc/fstab
   - R√©duire le swappiness (SSD) :
     echo 'vm.swappiness=10' >> /etc/sysctl.conf && sysctl -p
5. S√©curiser l'acc√®s Proxmox :
   - G√©n√©rer une paire de cl√©s SSH sur ta machine perso
   - Copier la cl√© publique : ssh-copy-id root@IP_PROXMOX
   - D√©sactiver l'authentification par mot de passe SSH
   - Configurer Fail2ban pour SSH + Web UI Proxmox
   - (Optionnel) Changer le port Web UI 8006 ‚Üí custom
```

#### PC 2 ‚Äî Serveur NAS/Backup Debian

```
T√¢ches :
1. Installer Debian 12 minimal (netinstall) sur le laptop i5-5200U
   - Pas d'environnement de bureau (serveur headless)
   - Partitionnement : /boot 512 Mo, swap 4 Go, / le reste (~995 Go)
   - Hostname : nas-backup.local
   - IP statique : 10.10.10.2/24 (m√™me r√©seau que Proxmox pour les backups)
2. Post-installation :
   - apt update && apt upgrade -y
   - Installer les essentiels :
     apt install -y nfs-kernel-server rsync gpg fail2ban ufw htop smartmontools
   - Configurer SMART monitoring (HDD de 1 To, surveiller la sant√©) :
     smartctl -a /dev/sda   # v√©rification initiale
     systemctl enable --now smartd
   - Cr√©er la structure de stockage :
     mkdir -p /backup/{vzdump/{daily,weekly},databases,configs}
     chown -R nobody:nogroup /backup
   - Configurer NFS :
     cat >> /etc/exports << 'EOF'
     /backup/vzdump   10.10.10.0/24(rw,sync,no_subtree_check,no_root_squash)
     /backup/databases 10.10.10.0/24(rw,sync,no_subtree_check,no_root_squash)
     /backup/configs  10.10.10.0/24(rw,sync,no_subtree_check,no_root_squash)
     EOF
     exportfs -ra && systemctl enable --now nfs-kernel-server
   - Configurer UFW : autoriser SSH + NFS uniquement depuis 10.10.10.0/24
     ufw default deny incoming
     ufw default allow outgoing
     ufw allow from 10.10.10.0/24 to any port 22
     ufw allow from 10.10.10.0/24 to any port 2049
     ufw allow from 10.10.10.0/24 to any port 111
     ufw enable
   - SSH hardening identique au PC 1
3. Monter le NFS sur Proxmox :
   - Datacenter > Storage > Add > NFS
   - Server : 10.10.10.2, Export : /backup/vzdump
   - Content : VZDump backup file
4. Configurer un cron de surveillance disque sur PC 2 :
   - Script v√©rifiant l'espace libre (alerte si < 20%)
   - Rotation automatique des vieux backups
```

**Livrables GitHub** :
- `docs/00-hardware-setup.md`
- `scripts/00-proxmox-postinstall.sh`
- `scripts/00-nas-setup.sh`

---

### Phase 1 : R√©seau & Segmentation (pfSense)

```
T√¢ches :
1. Configurer le bridge VLAN-aware sur Proxmox :
   - √âditer /etc/network/interfaces :
     auto vmbr0
     iface vmbr0 inet static
         address 10.10.10.1/24
         bridge-ports enpXs0    # adapter au nom r√©el de l'interface
         bridge-stp off
         bridge-fd 0
         bridge-vlan-aware yes
         bridge-vids 10 20 30 40

2. Cr√©er la VM pfSense (optimis√©e pour 8 Go total) :
   - 1 vCPU, 512 Mo RAM, 8 Go disque virtio
   - NIC 1 : vmbr0 (WAN ‚Äî acc√®s internet, pas de VLAN tag)
   - NIC 2 : vmbr0 VLAN trunk (LAN ‚Äî toutes les VLANs)
   - Installer pfSense CE depuis l'ISO
   - Assigner WAN (vtnet0) et LAN (vtnet1)

3. Configurer les VLANs dans pfSense :
   - VLAN 10 : Management    ‚Äî 10.10.10.0/24 ‚Äî GW: 10.10.10.254
   - VLAN 20 : Services      ‚Äî 10.10.20.0/24 ‚Äî GW: 10.10.20.254
   - VLAN 30 : CI/CD         ‚Äî 10.10.30.0/24 ‚Äî GW: 10.10.30.254
   - VLAN 40 : DMZ isol√©e    ‚Äî 10.10.40.0/24 ‚Äî GW: 10.10.40.254

4. R√®gles firewall inter-VLAN :
   - VLAN 10 (Management) ‚Üí acc√®s total √† tous les VLANs (admin)
   - VLAN 20 ‚Üî VLAN 30 : ports sp√©cifiques uniquement
     * TCP 80, 443 (HTTP/HTTPS)
     * TCP 22 (SSH)
     * TCP 3000 (Forgejo Web UI)
     * TCP 9090 (VictoriaMetrics)
     * TCP 3100 (Grafana)
     * TCP 8000 (Woodpecker Web UI)
   - VLAN 40 ‚Üí BLOCK vers VLAN 10, 20, 30 (isolation totale)
   - VLAN 40 ‚Üí ALLOW internet uniquement (pour updates Kali)
   - Tous les VLANs ‚Üí ALLOW DNS vers pfSense (port 53)

5. Services pfSense :
   - DHCP par VLAN (plages .100 √† .200)
   - DNS Resolver (Unbound) activ√© ‚Äî r√©solution locale *.lab.local
   - NAT outbound automatique pour l'acc√®s internet

6. V√©rification :
   - Depuis un CT en VLAN 20, ping 10.10.30.X ‚Üí OK (services autoris√©s)
   - Depuis VLAN 40, ping 10.10.20.X ‚Üí TIMEOUT (bloqu√©)
   - Depuis tous les VLANs, ping 8.8.8.8 ‚Üí OK (internet)
```

**Livrables GitHub** :
- `docs/01-network-architecture.md`
- `docs/diagrams/network-topology.png` (Excalidraw ou draw.io)
- `pfsense/firewall-rules-export.xml`

---

### Phase 2 : Services (VLAN 20)

```
T√¢ches :

1. Reverse Proxy Nginx ‚Äî Container LXC Debian 12 (256 Mo RAM) :
   - Template : debian-12-standard
   - VLAN tag : 20, IP : 10.10.20.10/24
   - Installer Nginx :
     apt install nginx -y
   - Configurer comme reverse proxy pour les services internes :
     * grafana.lab.local   ‚Üí 10.10.20.30:3000
     * forgejo.lab.local   ‚Üí 10.10.30.10:3000
     * woodpecker.lab.local ‚Üí 10.10.30.20:8000
     * registry.lab.local  ‚Üí 10.10.30.30:5000
   - HTTPS avec certificats auto-sign√©s :
     openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
       -keyout /etc/ssl/private/lab.key -out /etc/ssl/certs/lab.crt \
       -subj "/CN=*.lab.local"
   - Alternative : mini CA interne avec mkcert pour √©viter les warnings navigateur

2. Base de donn√©es PostgreSQL ‚Äî Container LXC Debian 12 (512 Mo RAM) :
   - VLAN tag : 20, IP : 10.10.20.20/24
   - Installer PostgreSQL 16 :
     sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt bookworm-pgdg main" \
       > /etc/apt/sources.list.d/pgdg.list'
     wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor \
       > /etc/apt/trusted.gpg.d/pgdg.gpg
     apt update && apt install -y postgresql-16
   - Configuration s√©curis√©e :
     * Cr√©er un utilisateur applicatif (pas postgres) :
       CREATE USER appuser WITH PASSWORD 'CHANGE_ME';
       CREATE DATABASE appdb OWNER appuser;
     * pg_hba.conf : n'accepter que les connexions depuis les VLANs autoris√©s :
       host all appuser 10.10.20.0/24 scram-sha-256
       host all appuser 10.10.30.0/24 scram-sha-256
     * postgresql.conf :
       listen_addresses = '10.10.20.20'
       shared_buffers = 128MB        # ajust√© pour 512 Mo RAM
       work_mem = 4MB
       effective_cache_size = 256MB
   - Script de backup quotidien :
     pg_dump -U postgres appdb | gzip > /backup/db_$(date +%Y%m%d).sql.gz
     Rotation : garder 7 jours, envoyer vers PC 2 via rsync

3. Monitoring ‚Äî Container LXC Debian 12 (512 Mo RAM) :
   - VLAN tag : 20, IP : 10.10.20.30/24

   a) VictoriaMetrics (remplace Prometheus) :
     - T√©l√©charger le binaire single-node depuis github.com/VictoriaMetrics/VictoriaMetrics/releases
     - Lancer avec param√®tres optimis√©s pour homelab :
       ./victoria-metrics-prod \
         -storageDataPath=/var/lib/victoria-metrics \
         -retentionPeriod=30d \
         -memory.allowedPercent=40 \
         -httpListenAddr=:8428
     - Configurer le scrape via -promscrape.config=prometheus.yml :
       scrape_configs:
         - job_name: 'node'
           static_configs:
             - targets:
               - '10.10.10.1:9100'   # Proxmox host
               - '10.10.20.10:9100'  # Nginx
               - '10.10.20.20:9100'  # PostgreSQL
               - '10.10.30.10:9100'  # Forgejo
               - '10.10.30.20:9100'  # Woodpecker
               - '10.10.30.30:9100'  # Registry
         - job_name: 'postgres'
           static_configs:
             - targets: ['10.10.20.20:9187']
     - PromQL 100% compatible ‚Äî les dashboards Grafana existants fonctionnent tels quels

   b) Grafana :
     - apt install -y grafana
     - Datasource : type Prometheus, URL http://localhost:8428
       (VictoriaMetrics expose une API compatible Prometheus)
     - Dashboards pr√©configur√©s :
       ‚Üí Node Exporter Full (ID: 1860)
       ‚Üí PostgreSQL (ID: 9628)
       ‚Üí VictoriaMetrics Single (ID: 10229)
     - Alertes :
       ‚Üí Disque > 80% sur n'importe quel n≈ìud
       ‚Üí Service down (target unreachable > 2 min)
       ‚Üí RAM > 85% sur le host Proxmox
       ‚Üí Backup NAS manquant (custom metric via node_exporter textfile)
     - Notifications : webhook Discord/Telegram

   c) Node Exporter sur CHAQUE CT/VM :
     apt install -y prometheus-node-exporter
     # Active par d√©faut sur :9100
```

**Livrables GitHub** :
- `ansible/roles/webserver/` (Nginx reverse proxy)
- `ansible/roles/database/` (PostgreSQL)
- `ansible/roles/monitoring/` (VictoriaMetrics + Grafana)
- `docs/02-services-deployment.md`

---

### Phase 3 : Pipeline CI/CD (VLAN 30)

```
T√¢ches :

1. Forgejo ‚Äî Container LXC Debian 12 (384 Mo RAM) :
   - VLAN tag : 30, IP : 10.10.30.10/24
   - Installation via binaire (plus l√©ger que Docker dans un CT) :
     # T√©l√©charger la derni√®re release depuis codeberg.org/forgejo/forgejo/releases
     wget https://codeberg.org/forgejo/forgejo/releases/download/vX.Y.Z/forgejo-X.Y.Z-linux-amd64
     chmod +x forgejo-* && mv forgejo-* /usr/local/bin/forgejo
   - Configurer avec SQLite (√©conomie de RAM, pas besoin d'un PostgreSQL d√©di√©)
   - Cr√©er un user syst√®me et un service systemd :
     adduser --system --shell /bin/bash --group --home /home/forgejo forgejo
   - app.ini ‚Äî config optimis√©e pour homelab :
     [server]
     ROOT_URL = https://forgejo.lab.local
     HTTP_PORT = 3000
     [database]
     DB_TYPE = sqlite3
     [cache]
     ADAPTER = memory
     [session]
     PROVIDER = memory
   - Configurer un miroir bidirectionnel avec GitHub :
     * New Migration ‚Üí GitHub ‚Üí URL du repo ‚Üí Mirror
   - Configurer le webhook vers Woodpecker CI :
     * Settings ‚Üí Webhooks ‚Üí Add ‚Üí URL: http://10.10.30.20:8000/hook

2. Woodpecker CI ‚Äî Container LXC Debian 12 (384 Mo RAM) :
   - VLAN tag : 30, IP : 10.10.30.20/24
   - Pr√©requis : installer Docker CE dans le CT
     (Activer les features nesting + keyctl dans les options du CT Proxmox)
   - Installation Woodpecker Server + Agent :
     # Server
     docker run -d --name=woodpecker-server \
       -e WOODPECKER_HOST=http://10.10.30.20:8000 \
       -e WOODPECKER_OPEN=true \
       -e WOODPECKER_FORGEJO=true \
       -e WOODPECKER_FORGEJO_URL=http://10.10.30.10:3000 \
       -e WOODPECKER_FORGEJO_CLIENT=xxx \
       -e WOODPECKER_FORGEJO_SECRET=xxx \
       -e WOODPECKER_AGENT_SECRET=agent-shared-secret \
       -p 8000:8000 \
       woodpeckerci/woodpecker-server:latest

     # Agent (sur le m√™me CT)
     docker run -d --name=woodpecker-agent \
       -e WOODPECKER_SERVER=10.10.30.20:9000 \
       -e WOODPECKER_AGENT_SECRET=agent-shared-secret \
       -e WOODPECKER_MAX_WORKFLOWS=2 \
       -v /var/run/docker.sock:/var/run/docker.sock \
       woodpeckerci/woodpecker-agent:latest

   - Pipeline de d√©mo (.woodpecker.yml) :
     steps:
       - name: test
         image: alpine
         commands:
           - echo "Running tests..."

       - name: lint
         image: golangci/golangci-lint
         commands:
           - golangci-lint run

       - name: build
         image: docker
         commands:
           - docker build -t 10.10.30.30:5000/myapp:${CI_COMMIT_SHA:0:8} .
           - docker push 10.10.30.30:5000/myapp:${CI_COMMIT_SHA:0:8}
         volumes:
           - /var/run/docker.sock:/var/run/docker.sock

       - name: notify
         image: alpine/curl
         commands:
           - 'curl -X POST $DISCORD_WEBHOOK -H "Content-Type: application/json" -d "{\"content\": \"‚úÖ Build OK: ${CI_REPO} @ ${CI_COMMIT_SHA:0:8}\"}"'
         when:
           - status: [success, failure]

3. Docker Registry ‚Äî Container LXC Debian 12 (256 Mo RAM) :
   - VLAN tag : 30, IP : 10.10.30.30/24
   - D√©ployer Registry v2 :
     docker run -d -p 5000:5000 \
       -v /data/registry:/var/lib/registry \
       --name registry registry:2
   - Authentification basique :
     mkdir /auth
     docker run --rm --entrypoint htpasswd httpd:2 -Bbn admin CHANGE_ME > /auth/htpasswd
     # Relancer avec :
     docker run -d -p 5000:5000 \
       -v /data/registry:/var/lib/registry \
       -v /auth:/auth \
       -e REGISTRY_AUTH=htpasswd \
       -e REGISTRY_AUTH_HTPASSWD_REALM="Lab Registry" \
       -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
       --name registry registry:2
   - Garbage collection planifi√©e (cron hebdomadaire) :
     0 3 * * 0 docker exec registry bin/registry garbage-collect /etc/docker/registry/config.yml
   - Configurer tous les CTs Docker pour faire confiance au registry :
     /etc/docker/daemon.json ‚Üí {"insecure-registries": ["10.10.30.30:5000"]}
```

**Livrables GitHub** :
- `ansible/roles/cicd/` (Forgejo + Woodpecker + Registry)
- `.woodpecker.yml` (pipeline de r√©f√©rence)
- `docker-compose/forgejo/docker-compose.yml`
- `docker-compose/woodpecker/docker-compose.yml`
- `docker-compose/registry/docker-compose.yml`
- `docs/03-cicd-pipeline.md`

---

### Phase 4 : S√©curit√© & Hardening

```
T√¢ches :

1. SSH Hardening (toutes les machines ‚Äî PC 1, PC 2, tous les CTs/VMs) :
   - /etc/ssh/sshd_config :
     PermitRootLogin no              # (sauf Proxmox host si besoin)
     PasswordAuthentication no
     PubkeyAuthentication yes
     Port 2222                        # Port custom
     MaxAuthTries 3
     AllowUsers tom                   # Ton user uniquement
     ClientAliveInterval 300
     ClientAliveCountMax 2
     X11Forwarding no
     AllowTcpForwarding no           # sauf si tunnel n√©cessaire
   - Fail2ban :
     apt install fail2ban -y
     Jail SSH : maxretry=3, bantime=3600, findtime=600
     Jail Proxmox Web UI : surveiller /var/log/daemon.log
     Jail custom pour Forgejo :
       [forgejo]
       enabled = true
       filter = forgejo
       logpath = /home/forgejo/log/forgejo.log
       maxretry = 5
       bantime = 3600

2. Firewall local sur chaque machine :
   - UFW sur chaque CT/VM (en plus de pfSense) :
     ufw default deny incoming
     ufw default allow outgoing
     ufw allow from 10.10.10.0/24 to any port 2222  # SSH depuis management
     ufw allow [PORT_SERVICE]                         # Port du service sp√©cifique
     ufw enable
   - Principe : double couche ‚Äî pfSense (p√©rim√®tre) + UFW (local)

3. Acc√®s distant s√©curis√© :
   - Installer Tailscale sur le host Proxmox :
     curl -fsSL https://tailscale.com/install.sh | sh
     tailscale up --advertise-routes=10.10.10.0/24,10.10.20.0/24,10.10.30.0/24
   - Depuis ton PC perso/mobile : acc√®s √† Proxmox Web UI + SSH via Tailscale
   - NE PAS advertiser le VLAN 40 (DMZ) via Tailscale
   - MFA sur Proxmox Web UI :
     Datacenter > Permissions > Two Factor > TOTP
     Ajouter TOTP pour ton compte

4. Audit & Logs centralis√©s :
   - Option A ‚Äî rsyslog (l√©ger, recommand√© pour 8 Go) :
     * Sur chaque CT/VM, configurer rsyslog :
       *.* @10.10.20.30:514
     * Sur le CT monitoring, recevoir avec rsyslog :
       module(load="imudp")
       input(type="imudp" port="514")
     * Logrotate :
       /var/log/*.log { daily, rotate 7, compress, missingok }

   - Option B ‚Äî Promtail + Loki (si la RAM le permet) :
     * Plus riche (labels, recherche dans Grafana)
     * Mais consomme ~100-150 Mo de RAM en plus
     * √Ä envisager seulement si le budget RAM le permet

5. Zone de test pentest (VLAN 40) :
   - VM Kali Linux ‚Äî 2 Go RAM ‚Äî ‚ö° √âTEINTE PAR D√âFAUT :
     qm create 200 --name kali-pentest --memory 2048 --cores 2 \
       --net0 virtio,bridge=vmbr0,tag=40 --cdrom local:iso/kali.iso \
       --ostype l26 --scsihw virtio-scsi-pci --boot order=scsi0
   - D√©marrer uniquement pour les tests :
     qm start 200
   - V√©rifier l'isolation AVANT tout test :
     # Depuis Kali :
     nmap -sn 10.10.10.0/24   ‚Üí R√©sultat attendu : 0 hosts up
     nmap -sn 10.10.20.0/24   ‚Üí R√©sultat attendu : 0 hosts up
     nmap -sn 10.10.30.0/24   ‚Üí R√©sultat attendu : 0 hosts up
     ping 8.8.8.8              ‚Üí OK (internet autoris√©)
     curl https://kali.org     ‚Üí OK (internet autoris√©)
   - Documenter les r√©sultats de scan comme preuve d'isolation
   - Exporter les r√©sultats nmap en format XML pour le rapport :
     nmap -sn 10.10.10.0/24 -oX /tmp/scan-vlan10.xml
```

**Livrables GitHub** :
- `ansible/roles/hardening/` (SSH, Fail2ban, UFW)
- `docs/04-security-audit.md` (avec screenshots des scans Kali)

---

### Phase 5 : Automatisation & Infrastructure as Code

```
T√¢ches :

1. Scripts Bash :
   - scripts/00-proxmox-postinstall.sh :
     * D√©sactivation repo enterprise
     * Activation repo no-subscription
     * Suppression popup souscription
     * Mise √† jour syst√®me
     * Configuration swap + swappiness
     * Installation paquets utiles (vim, htop, curl, git, fail2ban, tmux)
     * Hardening SSH de base
     * Affichage d'un r√©sum√© post-install (RAM, CPU, disque)

   - scripts/00-nas-setup.sh :
     * Installation paquets NFS + rsync + gpg + smartmontools
     * Cr√©ation arborescence /backup/
     * Configuration exports NFS
     * Configuration UFW
     * Configuration SMART monitoring
     * Cr√©ation cron de nettoyage des vieux backups
     * Test d'√©criture NFS depuis le script

   - scripts/backup/backup-all-vms.sh :
     * vzdump de toutes les VMs/CTs actives
     * Compression zstd (meilleur ratio que lzo, moins de CPU que gzip)
     * rsync vers PC 2 (NAS) avec --partial (reprise en cas de coupure)
     * Rotation : 7 daily, 4 weekly
     * Notification en cas d'√©chec (webhook Discord ou email via msmtp)
     * Log dans un fichier + envoi m√©trique vers node_exporter textfile

   - scripts/backup/backup-databases.sh :
     * pg_dump de toutes les bases
     * Compression gzip
     * Chiffrement GPG (cl√© g√©n√©r√©e au setup)
     * Envoi vers PC 2 via rsync
     * Rotation 7 jours
     * V√©rification d'int√©grit√© (gunzip -t)

   - scripts/health-check.sh :
     * V√©rifie que chaque CT/VM est running (qm/pct status)
     * V√©rifie l'espace disque (alerte si > 80%)
     * V√©rifie la RAM libre du host (alerte si < 15%)
     * V√©rifie que les services critiques r√©pondent :
       curl -sf http://10.10.20.30:3000/api/health  # Grafana
       curl -sf http://10.10.30.10:3000              # Forgejo
       curl -sf http://10.10.30.20:8000              # Woodpecker
     * V√©rifie la connectivit√© NAS (ping + mount NFS)
     * V√©rifie la sant√© SMART du disque NAS
     * Sortie JSON pour int√©gration monitoring
     * Code retour non-z√©ro si probl√®me critique

2. Ansible :
   - Inventaire structur√© par VLAN :
     inventory/
     ‚îú‚îÄ‚îÄ group_vars/
     ‚îÇ   ‚îú‚îÄ‚îÄ all.yml           # Variables globales (DNS, NTP, SSH port, domain)
     ‚îÇ   ‚îú‚îÄ‚îÄ services.yml      # Variables VLAN 20
     ‚îÇ   ‚îú‚îÄ‚îÄ cicd.yml          # Variables VLAN 30
     ‚îÇ   ‚îî‚îÄ‚îÄ dmz.yml           # Variables VLAN 40
     ‚îî‚îÄ‚îÄ hosts.ini :
         [management]
         proxmox    ansible_host=10.10.10.1
         nas        ansible_host=10.10.10.2

         [services]
         nginx      ansible_host=10.10.20.10
         postgresql ansible_host=10.10.20.20
         monitoring ansible_host=10.10.20.30

         [cicd]
         forgejo    ansible_host=10.10.30.10
         woodpecker ansible_host=10.10.30.20
         registry   ansible_host=10.10.30.30

         [dmz]
         kali       ansible_host=10.10.40.10

   - Playbooks :
     * site.yml ‚Äî D√©ploie TOUT (orchestrateur principal)
     * playbooks/webserver.yml ‚Äî Nginx reverse proxy
     * playbooks/database.yml ‚Äî PostgreSQL + backup
     * playbooks/monitoring.yml ‚Äî VictoriaMetrics + Grafana + Node Exporter
     * playbooks/cicd.yml ‚Äî Forgejo + Woodpecker + Registry
     * playbooks/hardening.yml ‚Äî SSH + Fail2ban + UFW sur tout
     * playbooks/nas.yml ‚Äî Configuration du PC 2

   - R√¥les r√©utilisables :
     * common/ ‚Äî Mise √† jour, paquets de base, NTP, locales, Node Exporter
     * hardening/ ‚Äî SSH config, Fail2ban, UFW
     * webserver/ ‚Äî Nginx + reverse proxy config + certificats
     * database/ ‚Äî PostgreSQL + pg_hba + tuning + backup cron
     * monitoring/ ‚Äî VictoriaMetrics + Grafana + dashboards
     * cicd/ ‚Äî Forgejo + Woodpecker CI + Registry
     * backup/ ‚Äî Scripts de backup + rotation + GPG

   - Ansible Vault pour les secrets :
     ansible-vault create inventory/group_vars/vault.yml
     # Stocker : mots de passe BDD, tokens API, secrets Woodpecker, cl√© GPG

3. Optionnel ‚Äî Terraform :
   - Provider : bpg/proxmox (activement maintenu)
   - D√©finir chaque CT/VM en HCL :
     * Ressources (RAM, CPU), r√©seau (VLAN tag), storage
   - terraform plan ‚Üí terraform apply pour recr√©er l'infra from scratch
   - Bon compl√©ment portfolio IaC m√™me si moins critique en homelab
   - Stocker le state en local (pas besoin de backend S3 pour un homelab)
```

**Livrables GitHub** :
- `scripts/` (tous les scripts Bash)
- `ansible/` (inventaire, playbooks, r√¥les, vault)
- `terraform/` (optionnel)
- `docs/05-automation-iac.md`

---

### Phase 6 : Backup & Disaster Recovery

```
T√¢ches :

1. Backup Proxmox natif ‚Üí PC 2 :
   - Configurer le storage NFS dans Proxmox :
     Datacenter > Storage > Add > NFS
     ID: nas-backup, Server: 10.10.10.2, Export: /backup/vzdump
     Content: VZDump backup file
   - Planifier via Datacenter > Backup :
     * Tous les jours √† 02:00 ‚Äî Mode snapshot ‚Äî Compression zstd
     * S√©lection : toutes les VMs/CTs (sauf Kali si √©teinte)
     * Storage : nas-backup (PC 2)
     * Retention : keep-daily=7, keep-weekly=4
     * Email notification : en cas d'√©chec uniquement

2. Backup bases de donn√©es ‚Üí PC 2 :
   - Cron sur le CT PostgreSQL (tous les jours √† 01:00) :
     0 1 * * * /usr/local/bin/backup-databases.sh >> /var/log/backup-db.log 2>&1
   - Le script :
     * pg_dump de chaque base
     * Compression gzip
     * V√©rification d'int√©grit√© : gunzip -t fichier.sql.gz
     * Chiffrement GPG (cl√© g√©n√©r√©e au setup initial)
     * rsync vers PC 2 : /backup/databases/
     * Suppression des dumps > 7 jours en local
     * √âcriture d'une m√©trique dans /var/lib/node_exporter/textfile/backup_db.prom :
       backup_db_last_success_timestamp <epoch>
       backup_db_last_size_bytes <size>
   - Sur PC 2, rotation :
     * Garder 7 daily + 4 weekly (script cron sur le NAS)

3. Backup configs :
   - Script hebdomadaire (dimanche 03:00) qui sauvegarde :
     * /etc/pve/ (config Proxmox ‚Äî cluster, storage, VM configs)
     * /etc/network/interfaces
     * Export XML des r√®gles pfSense (via l'API ou manuellement)
     * Liste des packages install√©s sur chaque CT :
       dpkg --get-selections > /backup/configs/CT_NAME-packages.txt
   - Destination : PC 2 /backup/configs/
   - Le repo Git Ansible constitue d√©j√† un backup de la config logique

4. Test de restauration (DOCUMENTER !) :
   - Proc√©dure test√©e : restaurer un CT depuis un backup vzdump
     qmrestore /mnt/pve/nas-backup/dump/vzdump-lxc-XXX.tar.zst NEWID --storage local
   - Proc√©dure test√©e : restaurer une base PostgreSQL
     gpg -d backup.sql.gz.gpg | gunzip | psql -U postgres newdb
   - Sc√©nario DR complet document√© :
     * Simuler la perte du CT Nginx
     * Restaurer depuis le backup sur PC 2
     * V√©rifier le service (curl http://IP:80)
     * Mesurer le temps de restauration (RTO)
     * V√©rifier la fra√Æcheur des donn√©es restaur√©es (RPO)
   - Documenter le RTO et RPO de chaque service dans un tableau :
     | Service | RPO | RTO estim√© | RTO mesur√© |
     |---------|-----|------------|------------|
     | Nginx   | 24h | 5 min      | X min      |
     | PostgreSQL | 24h | 10 min  | X min      |
     | ...     | ... | ...        | ...        |

5. Monitoring des backups :
   - M√©triques expos√©es via node_exporter textfile collector :
     * backup_vzdump_last_success_timestamp
     * backup_db_last_success_timestamp
     * backup_nas_disk_free_bytes
   - Alertes Grafana :
     * Backup vzdump manquant (derni√®re m√©trique > 26h)
     * Backup BDD manquant (derni√®re m√©trique > 26h)
     * Espace disque NAS < 20%
   - Dashboard Grafana d√©di√© "Backup Status" avec :
     * Derni√®re date de backup par type
     * Taille des backups (√©volution dans le temps)
     * Espace disque NAS restant
```

**Livrables GitHub** :
- `scripts/backup/` (tous les scripts)
- `docs/06-backup-dr.md` (proc√©dures + r√©sultats de test DR + tableau RTO/RPO)

---

## üìÅ Structure du repo GitHub

```
proxmox-secure-lab/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE (MIT)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ CHANGELOG.md
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ 00-hardware-setup.md
‚îÇ   ‚îú‚îÄ‚îÄ 01-network-architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ 02-services-deployment.md
‚îÇ   ‚îú‚îÄ‚îÄ 03-cicd-pipeline.md
‚îÇ   ‚îú‚îÄ‚îÄ 04-security-audit.md
‚îÇ   ‚îú‚îÄ‚îÄ 05-automation-iac.md
‚îÇ   ‚îú‚îÄ‚îÄ 06-backup-dr.md
‚îÇ   ‚îú‚îÄ‚îÄ DECISIONS.md              # Architecture Decision Records (ADR)
‚îÇ   ‚îî‚îÄ‚îÄ diagrams/
‚îÇ       ‚îú‚îÄ‚îÄ network-topology.png
‚îÇ       ‚îú‚îÄ‚îÄ architecture-overview.png
‚îÇ       ‚îî‚îÄ‚îÄ ram-budget.png
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 00-proxmox-postinstall.sh
‚îÇ   ‚îú‚îÄ‚îÄ 00-nas-setup.sh
‚îÇ   ‚îú‚îÄ‚îÄ backup/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup-all-vms.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup-databases.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup-configs.sh
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ verify-backups.sh
‚îÇ   ‚îî‚îÄ‚îÄ health-check.sh
‚îÇ
‚îú‚îÄ‚îÄ ansible/
‚îÇ   ‚îú‚îÄ‚îÄ ansible.cfg
‚îÇ   ‚îú‚îÄ‚îÄ inventory/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hosts.ini
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ group_vars/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ all.yml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ services.yml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cicd.yml
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dmz.yml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ vault.yml          # Secrets chiffr√©s Ansible Vault
‚îÇ   ‚îú‚îÄ‚îÄ site.yml
‚îÇ   ‚îú‚îÄ‚îÄ playbooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ webserver.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cicd.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hardening.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nas.yml
‚îÇ   ‚îî‚îÄ‚îÄ roles/
‚îÇ       ‚îú‚îÄ‚îÄ common/
‚îÇ       ‚îú‚îÄ‚îÄ hardening/
‚îÇ       ‚îú‚îÄ‚îÄ webserver/
‚îÇ       ‚îú‚îÄ‚îÄ database/
‚îÇ       ‚îú‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ cicd/
‚îÇ       ‚îî‚îÄ‚îÄ backup/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose/
‚îÇ   ‚îú‚îÄ‚îÄ forgejo/docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ woodpecker/docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/docker-compose.yml    # Alternative Docker au d√©ploiement natif
‚îÇ   ‚îî‚îÄ‚îÄ registry/docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ terraform/ (optionnel)
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
‚îÇ
‚îú‚îÄ‚îÄ pfsense/
‚îÇ   ‚îî‚îÄ‚îÄ firewall-rules-export.xml
‚îÇ
‚îî‚îÄ‚îÄ .woodpecker.yml
```

---

## üìù Template README.md pour GitHub

```markdown
# üîí Proxmox Secure Lab

> Environnement de test s√©curis√© et segment√© sur Proxmox VE,
> d√©ploy√© sur deux laptops recycl√©s ‚Äî Infrastructure as Code.

## üéØ Objectif

Construire un homelab professionnel simulant un environnement de production
avec segmentation r√©seau, CI/CD, monitoring, hardening s√©curit√© et disaster recovery.

## üíª Hardware

| Machine | CPU | RAM | Stockage | R√¥le |
|---------|-----|-----|----------|------|
| PC 1 | i5-8265U (4C/8T) | 8 Go | 250 Go SSD | Proxmox VE ‚Äî Hyperviseur |
| PC 2 | i5-5200U (2C/4T) | 8 Go | 1 To HDD | Debian ‚Äî NAS / Backup |

## üèóÔ∏è Architecture

![Architecture](docs/diagrams/architecture-overview.png)

| VLAN | R√©seau | R√¥le | Services |
|------|--------|------|----------|
| 10 | 10.10.10.0/24 | Management | Proxmox UI, SSH, NAS |
| 20 | 10.10.20.0/24 | Services | Nginx, PostgreSQL, Grafana |
| 30 | 10.10.30.0/24 | CI/CD | Forgejo, Woodpecker CI, Registry |
| 40 | 10.10.40.0/24 | DMZ | Kali Linux (isol√©) |

## üõ†Ô∏è Stack technique

- **Hyperviseur** : Proxmox VE 8.x (ext4)
- **Firewall** : pfSense CE (VM, 512 Mo)
- **Automatisation** : Ansible + Bash + Terraform (optionnel)
- **CI/CD** : Forgejo + Woodpecker CI + Docker Registry v2
- **Monitoring** : VictoriaMetrics + Grafana + Node Exporter
- **Backup** : vzdump ‚Üí rsync chiffr√© GPG ‚Üí NAS (PC 2)
- **S√©curit√©** : VLANs, pfSense, Fail2ban, UFW, Tailscale, MFA

## ‚ö° Optimisation 8 Go RAM

Architecture optimis√©e avec **7 containers LXC + 2 VMs** pour tenir dans 8 Go :
- Containers LXC pour tous les services (10-50√ó plus l√©ger qu'une VM)
- VictoriaMetrics au lieu de Prometheus (30-50% moins gourmand)
- VM Kali √©teinte par d√©faut, d√©marr√©e √† la demande
- Marge de 2,2 Go sans Kali ‚Äî swap 4 Go en filet de s√©curit√©

D√©tail du budget RAM : voir [docs/00-hardware-setup.md](docs/00-hardware-setup.md)

## üöÄ D√©ploiement rapide

\`\`\`bash
# 1. Post-installation Proxmox
bash scripts/00-proxmox-postinstall.sh

# 2. Setup NAS (PC 2)
bash scripts/00-nas-setup.sh

# 3. D√©ployer toute l'infrastructure
cd ansible/
ansible-playbook -i inventory/hosts.ini site.yml --ask-vault-pass
\`\`\`

## üìö Documentation

| Phase | Document | Contenu |
|-------|----------|---------|
| 0 | [Hardware Setup](docs/00-hardware-setup.md) | Installation Proxmox + NAS |
| 1 | [Network](docs/01-network-architecture.md) | VLANs, pfSense, firewall rules |
| 2 | [Services](docs/02-services-deployment.md) | Nginx, PostgreSQL, Monitoring |
| 3 | [CI/CD](docs/03-cicd-pipeline.md) | Forgejo, Woodpecker, Registry |
| 4 | [Security](docs/04-security-audit.md) | Hardening, scans, isolation |
| 5 | [Automation](docs/05-automation-iac.md) | Ansible, scripts, Terraform |
| 6 | [Backup & DR](docs/06-backup-dr.md) | Proc√©dures, RTO/RPO, tests |

## üèóÔ∏è Architecture Decision Records

Les choix techniques sont document√©s dans [docs/DECISIONS.md](docs/DECISIONS.md) :
- Pourquoi Forgejo plut√¥t que Gitea
- Pourquoi VictoriaMetrics plut√¥t que Prometheus
- Pourquoi ext4 plut√¥t que ZFS
- Pourquoi LXC plut√¥t que Docker partout

## üë§ Auteur

**Tom Daluzeau** ‚Äî Alternant Administrateur Syst√®mes & DevOps
- GitHub : [@ton-github](https://github.com/ton-github)
- LinkedIn : [Tom Daluzeau](https://linkedin.com/in/tom-daluzeau)
```

---

## üöÄ Commandes Git pour initialiser le projet

```bash
# Cr√©er le repo local
mkdir proxmox-secure-lab && cd proxmox-secure-lab
git init

# Cr√©er la structure compl√®te
mkdir -p docs/diagrams \
         scripts/backup \
         ansible/{inventory/group_vars,playbooks,roles/{common,hardening,webserver,database,monitoring,cicd,backup}} \
         docker-compose/{forgejo,woodpecker,monitoring,registry} \
         terraform \
         pfsense

# Cr√©er les fichiers de base
touch README.md LICENSE CHANGELOG.md .gitignore docs/DECISIONS.md

# .gitignore
cat << 'EOF' > .gitignore
# Secrets
*.key
*.pem
*.crt
*secret*
*password*
.env
vault.yml
!ansible/inventory/group_vars/vault.yml  # le fichier vault est chiffr√©, ok

# Terraform
*.tfstate
*.tfstate.backup
.terraform/
.terraform.lock.hcl

# Ansible
*.retry

# Backups
*.vzdump
*.lzo
*.zst

# OS
.DS_Store
Thumbs.db

# GPG
*.gpg
*.asc

# Logs
*.log

# Editor
.vscode/
.idea/
*.swp
*~
EOF

# Premier commit
git add .
git commit -m "üéâ init: project structure for Proxmox Secure Lab v3 (Forgejo + Woodpecker + VictoriaMetrics)"

# Lier √† GitHub
git remote add origin git@github.com:TON-USERNAME/proxmox-secure-lab.git
git branch -M main
git push -u origin main
```

---

## üí° Conseils pour le portfolio

1. **Commite souvent** avec des messages conventionnels :
   - `feat: deploy pfSense with 4 VLANs`
   - `feat: add Forgejo + Woodpecker CI pipeline`
   - `feat: add VictoriaMetrics monitoring stack`
   - `docs: add network topology diagram`
   - `fix: reduce Grafana memory to fit budget`
   - `docs: add architecture decision records`
2. **Documente les gal√®res** ‚Äî Les recruteurs adorent voir la r√©solution de probl√®mes
3. **DECISIONS.md** ‚Äî Un fichier ADR (Architecture Decision Records) qui explique *pourquoi* chaque choix technique. √áa montre une r√©flexion d'ing√©nieur, pas juste un tuto suivi.
4. **Ajoute des screenshots** dans `/docs/diagrams/` :
   - Proxmox dashboard avec les CTs/VMs et la conso RAM r√©elle
   - Grafana dashboards en fonctionnement
   - pfSense firewall rules
   - R√©sultat des scans Kali prouvant l'isolation VLAN
   - Pipeline Woodpecker en succ√®s
5. **√âcris un post LinkedIn** √† chaque phase termin√©e
6. **Tag les versions** :
   - `git tag v0.1-network` apr√®s Phase 1
   - `git tag v0.2-services` apr√®s Phase 2
   - `git tag v1.0-complete` quand tout fonctionne
7. **Mets en avant la contrainte RAM** ‚Äî Optimiser pour 8 Go montre une vraie comp√©tence de sizing
8. **Documente le budget RAM r√©el** (via `free -h` et screenshots Proxmox) vs le th√©orique
9. **Compare v2 ‚Üí v3** dans le CHANGELOG : montre que tu it√®res et modernises ta stack

---

## üìä Changelog v2 ‚Üí v3

| Changement | v2 | v3 | Raison |
|------------|----|----|--------|
| Git self-hosted | Gitea | **Forgejo** | Fork communautaire actif, gouvernance ind√©pendante |
| CI/CD | Drone CI | **Woodpecker CI** | Fork open source actif, Drone quasi-abandonn√© |
| Monitoring | Prometheus (768 Mo) | **VictoriaMetrics** (512 Mo) | -256 Mo RAM, meilleure compression, PromQL compatible |
| Filesystem | ext4 + config ZFS inutile | **ext4 propre** | Suppression config ZFS ARC qui ne servait √† rien |
| Marge RAM | 1,6 Go | **2,2 Go** | +600 Mo gr√¢ce aux optimisations stack |
| Secrets | Aucune gestion | **Ansible Vault** | Bonne pratique s√©curit√© |
| Backup monitoring | Basique | **M√©triques node_exporter** | Alertes Grafana automatiques si backup manquant |
| Disque NAS | Pas de monitoring | **SMART monitoring** | Surveillance sant√© du HDD |
| SSH hardening | Basique | **√âtendu** | +ClientAlive, +X11Forwarding off, +AllowTcpForwarding |
| Documentation | Docs phases | **+ DECISIONS.md (ADR)** | Architecture Decision Records pour le portfolio |
| Health check | Basique | **+ M√©triques JSON** | Int√©gration monitoring + code retour exploitable |
